{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c88b7bb-3e35-49da-af2c-783a1db69a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms.base import LLM\n",
    "from llama_index import LLMPredictor\n",
    "from typing import Optional, List, Mapping, Any\n",
    "from llama_index import SimpleDirectoryReader, LangchainEmbedding, ServiceContext, Document, VectorStoreIndex\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter,SpacyTextSplitter\n",
    "from llama_index.node_parser import SimpleNodeParser\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "!export PYTORCH_CUDA_ALLOC_CONF=\"0.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71077196-c213-427d-94b2-7588d2b454ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Loading checkpoint shards: 100%|██████████| 8/8 [00:08<00:00,  1.12s/it]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"/home/user/imported_models/chatglm-6b-20230419\",trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(\"/home/user/imported_models/chatglm-6b-20230419\", trust_remote_code=True).half().cuda()\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbde8f82-d0ab-42f3-a3f6-88303e7f6973",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLLM(LLM):\n",
    "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
    "        response, history = model.chat(tokenizer, prompt, history=[])\n",
    "        return response\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Mapping[str, Any]:\n",
    "        return {\"name_of_model\": \"chatglm-6b\"}\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"custom\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93609525-c431-43fa-a9df-f05c889cc6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_predictor = LLMPredictor(llm=CustomLLM())\n",
    "# text_splitter = CharacterTextSplitter(separator=\"\\n\\n\", chunk_size=100, chunk_overlap=20)\n",
    "# parser = SimpleNodeParser(text_splitter=text_splitter)\n",
    "# documents = SimpleDirectoryReader(input_files=['./datalevel.txt']).load_data()\n",
    "# nodes = parser.get_nodes_from_documents(documents)\n",
    "texts = open('./datalevel.txt', 'r', encoding='utf-8').read().split('\\n\\n')\n",
    "documents = [Document(text) for text in texts]\n",
    "embed_model = LangchainEmbedding(HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\"\n",
    "))\n",
    "service_context = ServiceContext.from_defaults(embed_model=embed_model, llm_predictor=llm_predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fab18d5e-ee3a-4b4a-ab8c-5db0c2e2e76a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "属性： 安全级别数字\n",
      "\n",
      "身份证号： 安全等级4\n",
      "\n",
      "吸烟史： 安全等级3\n",
      "\n",
      "是否患有糖尿病： 安全等级3\n"
     ]
    }
   ],
   "source": [
    "from llama_index import VectorStoreIndex\n",
    "index = VectorStoreIndex.from_documents(documents, service_context=service_context)\n",
    "query_engine = index.as_query_engine(similarity_top_k=5)\n",
    "query = \"请说明客户信息表中，身份证号，吸烟史，是否患有糖尿病等属性属于什么安全级别?按照\\\"属性：安全级别数字\\\"的方式输出\"\n",
    "result = query_engine.query(query)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a6ac9da3-1e6b-4e10-ad74-ad7af081267b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "手机号码 安全级别 3\n",
      "身份证号 安全级别 3\n",
      "吸烟史 安全级别 3\n",
      "是否患有糖尿病 安全级别 2\n"
     ]
    }
   ],
   "source": [
    "from llama_index import Prompt\n",
    "\n",
    "QA_PROMPT_TMPL = (\n",
    "    \"{context_str}\"\n",
    "    \"\\n\\n\"\n",
    "    \"根据以上信息，回答下面的问题：\"\n",
    "    \"Q: {query_str}\\n\"\n",
    "    )\n",
    "qa_template = Prompt(QA_PROMPT_TMPL)\n",
    "query_engine = index.as_query_engine(similarity_top_k=5, refine_template=qa_template)\n",
    "query = \"客户信息表（手机号码，身份证号，吸烟史，是否患有糖尿病）中的属性，安全级别都是多少？按照\\\"属性：安全级别数字\\\"的方式输出\"\n",
    "result = query_engine.query(query)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d29af4-96fa-4075-9513-4486f4d3ab76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
